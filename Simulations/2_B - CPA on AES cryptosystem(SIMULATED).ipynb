{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPA attack on AES cryptosystem\n",
    "In the previous notebook, we performed a DPA attack to find the secret key of the AES. We used one leakage bit per byte to find the key but we weren't always succesful. In the following notebook we will make use of all bits to try and uncover the key.\n",
    "\n",
    "**Goals:**\n",
    "* Learn what hamming weights are\n",
    "* Learn the realtion between hamming weights and power consumption\n",
    "* perform a CPA attack on AES\n",
    "\n",
    "#### Prerequisites\n",
    "- [x] *0_series* notebooks \n",
    "- [x] *1_series* notebooks\n",
    "- [x] *2_A - DPA on AES cryptosystem* notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming weights (HW)\n",
    "Let's again load traces collected from the chipwhisperer as we did in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "key = np.load('../src/sim_files/2B_1-key1.npy')\n",
    "trace_array = np.load('../src/sim_files/2B_2-trace_array1.npy')\n",
    "textin_array = np.load('../src/sim_files/2B_3-textin_array1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some code from the previous tutorial again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "numtraces = np.shape(trace_array)[0] #total number of traces\n",
    "numpoints = np.shape(trace_array)[1] #samples per trace\n",
    "\n",
    "sbox = [\n",
    "    # 0    1    2    3    4    5    6    7    8    9    a    b    c    d    e    f \n",
    "    0x63,0x7c,0x77,0x7b,0xf2,0x6b,0x6f,0xc5,0x30,0x01,0x67,0x2b,0xfe,0xd7,0xab,0x76, # 0\n",
    "    0xca,0x82,0xc9,0x7d,0xfa,0x59,0x47,0xf0,0xad,0xd4,0xa2,0xaf,0x9c,0xa4,0x72,0xc0, # 1\n",
    "    0xb7,0xfd,0x93,0x26,0x36,0x3f,0xf7,0xcc,0x34,0xa5,0xe5,0xf1,0x71,0xd8,0x31,0x15, # 2\n",
    "    0x04,0xc7,0x23,0xc3,0x18,0x96,0x05,0x9a,0x07,0x12,0x80,0xe2,0xeb,0x27,0xb2,0x75, # 3\n",
    "    0x09,0x83,0x2c,0x1a,0x1b,0x6e,0x5a,0xa0,0x52,0x3b,0xd6,0xb3,0x29,0xe3,0x2f,0x84, # 4\n",
    "    0x53,0xd1,0x00,0xed,0x20,0xfc,0xb1,0x5b,0x6a,0xcb,0xbe,0x39,0x4a,0x4c,0x58,0xcf, # 5\n",
    "    0xd0,0xef,0xaa,0xfb,0x43,0x4d,0x33,0x85,0x45,0xf9,0x02,0x7f,0x50,0x3c,0x9f,0xa8, # 6\n",
    "    0x51,0xa3,0x40,0x8f,0x92,0x9d,0x38,0xf5,0xbc,0xb6,0xda,0x21,0x10,0xff,0xf3,0xd2, # 7\n",
    "    0xcd,0x0c,0x13,0xec,0x5f,0x97,0x44,0x17,0xc4,0xa7,0x7e,0x3d,0x64,0x5d,0x19,0x73, # 8\n",
    "    0x60,0x81,0x4f,0xdc,0x22,0x2a,0x90,0x88,0x46,0xee,0xb8,0x14,0xde,0x5e,0x0b,0xdb, # 9\n",
    "    0xe0,0x32,0x3a,0x0a,0x49,0x06,0x24,0x5c,0xc2,0xd3,0xac,0x62,0x91,0x95,0xe4,0x79, # a\n",
    "    0xe7,0xc8,0x37,0x6d,0x8d,0xd5,0x4e,0xa9,0x6c,0x56,0xf4,0xea,0x65,0x7a,0xae,0x08, # b\n",
    "    0xba,0x78,0x25,0x2e,0x1c,0xa6,0xb4,0xc6,0xe8,0xdd,0x74,0x1f,0x4b,0xbd,0x8b,0x8a, # c\n",
    "    0x70,0x3e,0xb5,0x66,0x48,0x03,0xf6,0x0e,0x61,0x35,0x57,0xb9,0x86,0xc1,0x1d,0x9e, # d\n",
    "    0xe1,0xf8,0x98,0x11,0x69,0xd9,0x8e,0x94,0x9b,0x1e,0x87,0xe9,0xce,0x55,0x28,0xdf, # e\n",
    "    0x8c,0xa1,0x89,0x0d,0xbf,0xe6,0x42,0x68,0x41,0x99,0x2d,0x0f,0xb0,0x54,0xbb,0x16  # f\n",
    "]\n",
    "\n",
    "def aes_internal(inputdata, key):\n",
    "    return sbox[inputdata ^ key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that there is a relationship between power consumption and the microcontroller's internal data is that setting this data takes power. The more data that is set high, the greater the average power draw. It's not far fetched, then, that there should be some sort of consistant relationship between the number of bits set to 1, called the **Hamming weight** and the power consumed by doing so.\n",
    "\n",
    "Hamming weight, despite being a pretty simple idea, actually isn't trivial to calculate (see https://en.wikipedia.org/wiki/Hamming_weight#Efficient_implementation). You can write a function to do this, but in Python it's far easier to just convert to a string of bits and count the `\"1\"`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hamming_weight(n):\n",
    "    return bin(n).count(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, create a lookup table (aka do the calculation for each number between 0 and 255 and stick them in an array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW = [bin(n).count(\"1\") for n in range(0, 256)]\n",
    "assert HW[0x53] == 4\n",
    "print(\"✔️ OK to continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first issue with AES is that we don't know where the SBox operation is happening. It should be happening pretty close to the beginning (let's guess and say within the first 2000 samples). One thought is that we could group the traces by hamming weight and assign a colour to each one. If we plot that, we might be able to find a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import brewer\n",
    "\n",
    "output_notebook()\n",
    "p = figure()\n",
    "\n",
    "plot_start = 0\n",
    "plot_end = 2000\n",
    "xrange = range(len(trace_array[0]))[plot_start:plot_end]\n",
    "bnum = 0\n",
    "color_mapper = brewer['PRGn'][9]\n",
    "\n",
    "for tnum in range(len(trace_array)):\n",
    "    hw_of_byte = HW[aes_internal(textin_array[tnum][bnum], key[bnum])]\n",
    "    p.line(xrange, trace_array[tnum][plot_start:plot_end], line_color=color_mapper[hw_of_byte])\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloration between HW and power consumption\n",
    "Unfortunately, you'll probably find that this plot doesn't really tell us much; the part of power consumption associated with the SBox output is just too small to pick out. We could try averaging the hamming weight groups to make things more distinct, but that doesn't solve the fundamental issue of the SBox output being lost in the noise of everything else happening on the chip.\n",
    "\n",
    "Instead, let's approach this from a different angle. Really, what we want here is to remove the overall \"shape\" of the trace and just leave the signal from the SBox output. We could just pick a trace and subtract it from each group, but subtracting an average of all the traces instead will make the plot more distinct. Even better would be to have an even weighting between all of the hamming weight groups, since the extreme hamming weights (0 and 8) are far less common than the middle values, but this won't end up being super necessary (though you can still attempt this if you'd like). The plot will also be more distinct (and plot a lot faster) if we average all the hamming weight groups to remove any outliers as well. Try implementing this (we'll again handle the plotting for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "p = figure()\n",
    "\n",
    "hw_groups = [[], [], [], [], [], [], [], [], []]\n",
    "for tnum in range(len(trace_array)):\n",
    "    hw_of_byte = HW[aes_internal(textin_array[tnum][bnum], key[bnum])]\n",
    "    hw_groups[hw_of_byte].append(trace_array[tnum])\n",
    "hw_averages = np.array([np.average(hw_groups[hw], axis=0) for hw in range(9)])\n",
    "avg_trace = np.average(hw_averages, axis=0)\n",
    "\n",
    "xrange = range(len(trace_array[0]))[plot_start:plot_end]\n",
    "color_mapper = brewer['PRGn'][9]\n",
    "for hw in range(9):  \n",
    "    p.line(xrange, (hw_averages[hw]-avg_trace)[plot_start:plot_end], line_color=color_mapper[hw])\n",
    "    \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this, you should get a very distinct spot where the colours separate. This is where the SBox operation is occuring. In fact, it's probably distinct enough that you can choose the SBox loction solely by where the graph is largest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbox_loc = np.argmax(abs(hw_averages[0]-avg_trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know where the SBox operation is happening, plot the hamming weight averages by their hamming weight at `sboc_loc`.\n",
    "\n",
    "**HINT: You may want to convert your hw_averages to a numpy array to allow you to access by column. `hw_averages[:,sbox_loc]` will give you `hw_averages` at the sbox_loc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "p = figure(title=\"HW vs Voltage Measurement\")\n",
    "p.line(range(0, 9), hw_averages[:,sbox_loc], line_color=\"red\")\n",
    "p.xaxis.axis_label = \"Hamming Weight of Intermediate Value\"\n",
    "p.yaxis.axis_label = \"Average Value of Measurement\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the relationship is mostly linear, which probably won't come out of left field. It makes sense that setting 8 data bits will take roughly 8x the power that setting one does. \n",
    "\n",
    "You will likely also find that the slope of the relationship is negative, unless you're on the ChipWhisperer Nano. This happens for a good reason. If you remember how we are measuring the current into the device, you'll find out that the voltage will go DOWN for an INCREASE in current. You can see this in the following figure:\n",
    "\n",
    "```     \n",
    "        Rshunt\n",
    "(Vin)----v^v^v^----------||------(To ChipWhisperer)\n",
    "                |\n",
    "                |\n",
    "       (To microcontroller)\n",
    "```\n",
    "For the ChipWhisperer Nano, the slope is positive due to the presence of an inverting amplifier on the input of the measurement port.\n",
    "\n",
    "We are measuring the drop across the shunt resistor. An increase in the current causes a higher voltage across the resistor. When no current flows there is no drop across the resistor. But since we only measure a single end of the resistor, we see a higher voltage when no current flows.\n",
    "\n",
    "We can fix the slope by simply inverting the measurement direction (adding a - in front of the measurement).\n",
    "\n",
    "Now that we know where the SBox operation is happening, try going back to the original plot and zoom in to that section. Can you pick out the difference between the hamming weights now?\n",
    "\n",
    "### Conclusion\n",
    "You should now be reasonably convinced that there is a linear relationship between the hamming weight of data being set in a microcontroller and the power it consumes from doing so.\n",
    "\n",
    "In the next part, we'll see how this can be used to greatly improve over our DPA attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPA\n",
    "Our first step will be to send some plaintext to the target device and observe its power consumption during the encryption. The capture loop will be the same as in the DPA attack. This time, however, we'll only need 50 traces to recover the key, a major improvement over the last attack!\n",
    "```Python\n",
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ktp = cw.ktp.Basic()\n",
    "trace_array = []\n",
    "textin_array = []\n",
    "\n",
    "key, text = ktp.next()\n",
    "\n",
    "target.set_key(key)\n",
    "\n",
    "N = 50\n",
    "for i in trange(N, desc='Capturing traces'):\n",
    "    scope.arm()\n",
    "    \n",
    "    target.simpleserial_write('p', text)\n",
    "    \n",
    "    ret = scope.capture()\n",
    "    if ret:\n",
    "        print(\"Target timed out!\")\n",
    "        continue\n",
    "    \n",
    "    response = target.simpleserial_read('r', 16)\n",
    "    \n",
    "    trace_array.append(scope.get_last_trace())\n",
    "    textin_array.append(text)\n",
    "    \n",
    "    key, text = ktp.next()\n",
    "    \n",
    "trace_array = np.array(trace_array)\n",
    "```\n",
    "\n",
    "In this SIMULATED notebook we will again load the correct data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "key = np.load('../src/sim_files/2B_4-key2.npy')\n",
    "trace_array = np.load('../src/sim_files/2B_5-trace_array2.npy')\n",
    "textin_array = np.load('../src/sim_files/2B_6-textin_array2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing our Correlation Algorithm\n",
    "As with DPA we will try several guesses for each key_byte. To test how good our guess is we will be using a measurement called the Pearson correlation coefficient, which measures the linear correlation between two datasets. \n",
    "\n",
    "The actual algorithm is as follows for datasets $X$ and $Y$ of length $N$, with means of $\\bar{X}$ and $\\bar{Y}$, respectively:\n",
    "\n",
    "$$r = \\frac{cov(X, Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "$cov(X, Y)$ is the covariance of `X` and `Y` and can be calculated as follows:\n",
    "\n",
    "$$cov(X, Y) = \\sum_{n=1}^{N}[(Y_n - \\bar{Y})(X_n - \\bar{X})]$$\n",
    "\n",
    "$\\sigma_X$ and $\\sigma_Y$ are the standard deviation of the two datasets. This value can be calculated with the following equation:\n",
    "\n",
    "$$\\sigma_X = \\sqrt{\\sum_{n=1}^{N}(X_n - \\bar{X})^2}$$\n",
    "\n",
    "As you can see, the calulation is actually broken down pretty nicely into some smaller chunks that we can implement with some simple functions. While we could use a library to calculate all this stuff for us, being able to implement a mathematical algorithm in code is a useful skill to develop. \n",
    "\n",
    "To start, build the following functions:\n",
    "\n",
    "1. `mean(X)` to calculate the mean of a dataset\n",
    "1. `std_dev(X, X_bar)` to calculate the standard deviation of a dataset. We'll need to reuse the mean for the covariance, so it makes more sense to calculate it once and pass it in to each function\n",
    "1. `cov(X, X_bar, Y, Y_bar)` to calculate the covariance of two datasets. Again, we can just pass in the means we calculate for std_dev here.\n",
    "\n",
    "**HINT: You can use `np.sum(X, axis=0)` to replace all of the $\\sum$ from earlier. The argument `axis=0` will sum across columns, allowing us to use a single `mean`, `std_dev`, and `cov` call for the entire power trace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "###BEGIN SOLUTION###\n",
    "def mean(X):\n",
    "    return np.sum(X, axis=0)/len(X)\n",
    "\n",
    "def std_dev(X, X_bar):\n",
    "    return np.sqrt(np.sum((X-X_bar)**2, axis=0))\n",
    "\n",
    "def cov(X, X_bar, Y, Y_bar):\n",
    "    return np.sum((X-X_bar)*(Y-Y_bar), axis=0)\n",
    "###END SOLUTION###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check to make sure everything's as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[5, 3, 4, 4, 5, 6],\n",
    "             [27, 2, 3, 4, 12, 6],\n",
    "              [1, 3, 5, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6],\n",
    "             ]).transpose()\n",
    "a_bar = mean(a)\n",
    "b = np.array([[5, 4, 3, 2, 1, 3]]).transpose()\n",
    "b_bar = mean(b)\n",
    "\n",
    "o_a = std_dev(a, a_bar)\n",
    "o_b = std_dev(b, b_bar)\n",
    "\n",
    "ab_cov = cov(a, a_bar, b, b_bar)\n",
    "\n",
    "assert (a_bar == np.array([4.5, 9., 4., 3.5])).all()\n",
    "assert (b_bar == np.array([3.])).all()\n",
    "assert (o_a[3] > 4.1833001 and o_a[3] < 4.1833002)\n",
    "assert (o_b[0] > 3.162277 and o_b[0] < 3.162278)\n",
    "assert (ab_cov == np.array([-1., 28., -9., -10.])).all()\n",
    "print(\"✔️ OK to continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got all the building blocks to our correlation function, let's see if we can put everything together and break a single byte of AES. In order to do this, let's take a closer look at what we're trying to do and the data we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trace_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like the following:\n",
    "```python\n",
    "[\n",
    "    [point_0, point_1, point_2, ...], # trace 0\n",
    "    [point_0, point_1, point_2, ...], # trace 1\n",
    "    [point_0, point_1, point_2, ...], # trace 2\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "where the rows of the array are the different traces we captured and the columns of the array are the different points in those traces. The columns here will be one of the two datasets for our correlation equation. The other dataset will be the hamming weight of the SBox output:\n",
    "\n",
    "```python\n",
    "[\n",
    "      [HW[aes_internal(plaintext0[0], key[0])], # trace 0\n",
    "      [HW[aes_internal(plaintext1[0], key[0])], # trace 1\n",
    "      [HW[aes_internal(plaintext2[0], key[0])], # trace 2\n",
    "      ...\n",
    "]\n",
    "```\n",
    "\n",
    "which we'll shorten to:\n",
    "\n",
    "```python\n",
    "[\n",
    "      [hw], # trace 1\n",
    "      [hw], # trace 2\n",
    "      [hw], # trace 3\n",
    "      ...\n",
    "]\n",
    "```\n",
    "\n",
    "Like with the DPA attack, we don't know where the encryption is occurring, meaning we have to repeat the correlation calculation for each column in the trace array, with the largest correlation being our best guess for where the SBox output is happening. We obviously also don't know the key (that's the thing we're trying to find!), so we'll also need to repeat the best correlation calculation for each possible value of `key[0]` (0 to 255). The key with the highest absolute correlation is our best guess for the value of the key byte.\n",
    "\n",
    "A really nice feature of numpy is that we can do the correlation calculations across the entire trace at once (mean, std_dev, cov). That means there's no need to do:\n",
    "\n",
    "```python\n",
    "t_bar = []\n",
    "for point_num in range(len(trace_array[0])):\n",
    "    t_bar.append(mean(trace_array[:,point_num]))\n",
    "    # and so on...\n",
    "\n",
    "t_bar = np.array(t_bar)\n",
    "```\n",
    "\n",
    "when we can do\n",
    "\n",
    "```python\n",
    "t_bar = mean(trace_array)\n",
    "```\n",
    "\n",
    "and get the same thing back. The only caveat being that we need to make sure that the columns and rows of our arrays are the right way around (i.e. make sure your hamming weight array has 1 column and 50 rows and not the other way around). If you find it easier to construct and array one way and not the other, you can use the `.transpose()` method to swap the rows and columns.\n",
    "\n",
    "Once you've got all your correlations for a particular key guess, you want to find the largest absolute correlation. We're taking the absolute value of the correlation here since we only care that the relation between hamming weight and the power trace is linear, not that the slope is positive or negative. `max(abs(correlations))` will do that for you.\n",
    "\n",
    "Perform this for every possible value of the key byte (aka 0 to 255) and the one with the largest correlation is your best guess for the key. It's up to you how you want to extract this information from your loop, but one way of doing it is to stick the best guess for each of your key guesses in an array. Once you've gone through all the key guesses, you can extract the best guess with `np.argmax(maxcpa)` and the correlation of that guess with `max(maxcpa)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "maxcpa = [0] * 256\n",
    "\n",
    "# we don't need to redo the mean and std dev calculations \n",
    "# for each key guess\n",
    "t_bar = mean(trace_array) \n",
    "o_t = std_dev(trace_array, t_bar)\n",
    "\n",
    "for kguess in tnrange(0, 256):\n",
    "    hws = np.array([[HW[aes_internal(textin[0],kguess)] for textin in textin_array]]).transpose()\n",
    "    \n",
    "    # Add your code below this line\n",
    "    #raise NotImplementedError(\"Add your code here, and delete this.\")\n",
    "    \n",
    "    ###BEGIN SOLUTION###\n",
    "    hws_bar = mean(hws)\n",
    "    o_hws = std_dev(hws, hws_bar)\n",
    "    correlation = cov(trace_array, t_bar, hws, hws_bar)\n",
    "    cpaoutput = correlation/(o_t*o_hws)\n",
    "    maxcpa[kguess] = max(abs(cpaoutput))\n",
    "    \n",
    "\n",
    "guess = np.argmax(maxcpa)\n",
    "guess_corr = max(maxcpa)\n",
    "###END SOLUTION###\n",
    "print(\"Key guess: \", hex(guess))\n",
    "print(\"Correlation: \", guess_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we've recovered the byte correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert guess == 0x2b\n",
    "print(\"✔️ OK to continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break the rest of the key, simply repeat the attack for the rest of the bytes of the key. Don't forget to update your code from above to use the correct byte of the plaintext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_bar = np.sum(trace_array, axis=0)/len(trace_array)\n",
    "o_t = np.sqrt(np.sum((trace_array - t_bar)**2, axis=0))\n",
    "\n",
    "cparefs = [0] * 16 #put your key byte guess correlations here\n",
    "bestguess = [0] * 16 #put your key byte guesses here\n",
    "\n",
    "for bnum in tnrange(0, 16):\n",
    "    maxcpa = [0] * 256\n",
    "    for kguess in range(0, 256):\n",
    "        # Write your code below this line\n",
    "        ###BEGIN SOLUTION###\n",
    "        hws = np.array([[HW[aes_internal(textin[bnum],kguess)] for textin in textin_array]]).transpose()\n",
    "        hws_bar = mean(hws)\n",
    "        o_hws = std_dev(hws, hws_bar)\n",
    "        correlation = cov(trace_array, t_bar, hws, hws_bar)\n",
    "        cpaoutput = correlation/(o_t*o_hws)\n",
    "        maxcpa[kguess] = max(abs(cpaoutput))\n",
    "    bestguess[bnum] = np.argmax(maxcpa)\n",
    "    cparefs[bnum] = max(maxcpa)\n",
    "    ###END SOLUTION###\n",
    "\n",
    "\n",
    "print(\"Best Key Guess: \", end=\"\")\n",
    "for b in bestguess: print(\"%02x \" % b, end=\"\")\n",
    "print(\"\\n\", cparefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one final check to make sure you've got the correct key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bnum in range(16):\n",
    "    assert bestguess[bnum] == key[bnum], \\\n",
    "    \"Byte {} failed, expected {:02X} got {:02X}\".format(bnum, key[bnum], bestguess[bnum])\n",
    "print(\"✔️ OK to continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "You heave reached a grat checkpoint. There are 2 extra notebooks included that will teach you something abaout triggering and how you can circumvent jitter. The next two notebooks in the main tutorials series will have you implement some coutermeasures against our attacks and see if they have any effect.\n",
    "\n",
    "**Next notebook click here: [IN THE MAKING](./?.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
